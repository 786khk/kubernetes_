# kubernetes 직접설치


## 플레이그라운드
https://www.katacoda.com/courses/kube...
https://labs.play-with-k8s.com/


### 클러스터 초기화
k8s 플레이그라운드를 보면
아래와 같이 스크립트가 뜬다.


``` script
 You can bootstrap a cluster as follows:

 1. Initializes cluster master node: # 클러스터 초기화

 kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16


 2. Initialize cluster networking: # 클러스터 네트워크 구성

 kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml


 3. (Optional) Create an nginx deployment:

 kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx-app.yaml


                          The PWK team.

```

위 메세지가 뜨고나서 

```
kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
```

로 인스턴스를 새로 생성하면

Controller, scheduler, coreDNS, ETCD저장소 등을 만든다.

kubectl 
```shell
[node1 ~]$ kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
Initializing machine ID from random generator.
W1128 11:44:51.576439    1260 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/docker/containerd/containerd.sock". Please update your configuration!
I1128 11:44:51.872609    1260 version.go:256] remote version is much newer: v1.28.4; falling back to: stable-1.27
[init] Using Kubernetes version: v1.27.8
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.4.0-210-generic
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_BLKIO: enabled
        [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "", err: exit status 1
        [WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W1128 11:44:52.433108    1260 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.8, falling back to the nearest etcd version (3.5.7-0)
W1128 11:45:03.005463    1260 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local node1] and IPs [10.96.0.1 192.168.0.23]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost node1] and IPs [192.168.0.23 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost node1] and IPs [192.168.0.23 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
W1128 11:45:17.552041    1260 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.8, falling back to the nearest etcd version (3.5.7-0)
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 8.503555 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node node1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node node1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 41hupu.4b5auie7oowzfayz
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.23:6443 --token 41hupu.4b5auie7oowzfayz \
        --discovery-token-ca-cert-hash sha256:d9a06ccdbda0f12f79280ff9264ec2177dd1ce4bea3ef157665b80021cd1dda1 
Waiting for api server to startup
Warning: resource daemonsets/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
daemonset.apps/kube-proxy configured
No resources found

```


```shell
[node1 ~]$ kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
Initializing machine ID from random generator.
W1128 12:06:23.143270     439 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/docker/containerd/containerd.sock". Please update your configuration!
I1128 12:06:23.437594     439 version.go:256] remote version is much newer: v1.28.4; falling back to: stable-1.27
[init] Using Kubernetes version: v1.27.8
[preflight] Running pre-flight checks
        [WARNING CRI]: container runtime is not running: output: E1128 12:06:23.732661     454 remote_runtime.go:616] "Status from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
time="2023-11-28T12:06:23Z" level=fatal msg="getting status of runtime: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.4.0-210-generic
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_BLKIO: enabled
        [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "", err: exit status 1
        [WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W1128 12:06:23.865903     439 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.8, falling back to the nearest etcd version (3.5.7-0)
        [WARNING ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.27.8: output: E1128 12:06:24.054203     560 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/kube-apiserver:v1.27.8"
time="2023-11-28T12:06:24Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
        [WARNING ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.27.8: output: E1128 12:06:24.239032     614 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/kube-controller-manager:v1.27.8"
time="2023-11-28T12:06:24Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
        [WARNING ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.27.8: output: E1128 12:06:24.421193     673 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/kube-scheduler:v1.27.8"
time="2023-11-28T12:06:24Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
        [WARNING ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.27.8: output: E1128 12:06:24.598509     727 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/kube-proxy:v1.27.8"
time="2023-11-28T12:06:24Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
        [WARNING ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: E1128 12:06:24.791551     791 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/pause:3.9"
time="2023-11-28T12:06:24Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
        [WARNING ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.7-0: output: E1128 12:06:24.967981     852 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/etcd:3.5.7-0"
time="2023-11-28T12:06:24Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
        [WARNING ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: E1128 12:06:25.170219     909 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\"" image="registry.k8s.io/coredns/coredns:v1.10.1"
time="2023-11-28T12:06:25Z" level=fatal msg="pulling image: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/docker/containerd/containerd.sock: connect: no such file or directory\""
, error: exit status 1
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
error execution phase certs/ca: failure while saving ca certificate and key: couldn't write key: unable to write private key to file /etc/kubernetes/pki/ca.key: write /etc/kubernetes/pki/ca.key: no space left on device
To see the stack trace of this error execute with --v=5 or higher```

# 컨테이너 네트워크 인터페이스 지원

```
kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
```



```
[node1 ~]$ kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
configmap/kube-router-cfg created
daemonset.apps/kube-router created
serviceaccount/kube-router created
clusterrole.rbac.authorization.k8s.io/kube-router created
clusterrolebinding.rbac.authorization.k8s.io/kube-router created

```



# 워커 노드 연결

```
kubeadm join 192.168.0.23:6443 --token 41hupu.4b5auie7oowzfayz \
        --discovery-token-ca-cert-hash sha256:d9a06ccdbda0f12f79280ff9264ec2177dd1ce4bea3ef157665b80021cd1dda1
```


```
[node2 ~]$ kubeadm join 192.168.0.23:6443 --token 41hupu.4b5auie7oowzfayz \
>         --discovery-token-ca-cert-hash sha256:d9a06ccdbda0f12f79280ff9264ec2177dd1ce4bea3ef157665b80021cd1dda1
Initializing machine ID from random generator.
W1128 11:50:13.994052     436 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/docker/containerd/containerd.sock". Please update your configuration!
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.4.0-210-generic
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_BLKIO: enabled
        [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "", err: exit status 1
        [WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```
### kubectl 명령어 등록

- Cluster init 했을 때 설정파일 위치를 보고 설정파일을 열어본다
- `kubectl` 이라는 명령어를 쓸 수 있도록 등록(허가)

```
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

```


```
[node1 ~]$ cd /etc/kubernetes/
[node1 kubernetes]$ ls
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf
[node1 kubernetes]$ cat kubelet.conf 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1URXlPREV5TXpFeE4xb1hEVE16TVRFeU5URXlNekV4TjFvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTWdnCm1wVElSNy9JODFyVXl5dmVXaG51WElDUGN6bHM5amJNS3M1ZXcrazhqRy9ZcHdsNERjQzliM1U3bERXd1JtZFcKTWp1eFplNSsxMFpvSGYvb2hjc0V2MGtlUjU3ZTNTeXFubjVYa0gwVEdzZlZ0ZnZQdkh5SGQ2eE44T1N4RFRMMApKYWhpaWc1VzlYQ0VNdkxmK2I5cWRid2pqT0hvNUhQVXRuOEQ2cFZPT1E1aGFTN2lFdzE0dEpGRWNmZ2h3cGY4CkhrN2hYU1N3akNHSEYxUmdSanFpODcvUFpodG8zcDlFc0ZXTUV2M1VpNzNLejB3a2xtRWJpblVSeW92MS9xMUwKeFkxMUpRSUdwd2p4TE81UHExRTRJdkF4c0Z4bGVxS1Zpc2UzWUhmaTZmaXBLaktlOVRjd2MvVnRMRzJrdUpPKwpSNFp2UzJpODZzMXA2YlNVSUhVQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZJOGhsd3dnZVRqYWo2MUF6SStpdW0rZC9QTXJNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSTc5S2ZzWXlEemVLcWhkRGpQawpQaHp1RjYyek9kOHJ0bFB5REcrSEcvUFNkOU52Y0d1Z0R1cmRWUVZJa0R4UERMaGpFenF5NkF4RmRDMVh6ckM5CkNNSkhXN3NVYSt0Vm9tM0pncWVsOEZGQUFvWEpnNmp5cndMbFJ3emJmcXlLNVhGWEZpWWR0amN2RHJVSTdaSUwKWm41QTBKQW8xYjNodkpmZVBqY2cyaW1kQytrRjJ3YmVHcUpvOVpoaDArZ3VQMTEzOXZKNVFJdmI2aGQxUVVpNwo1TlhqMFNNMjVERHZJRmJSMElWZFpFNEViYVY4bjhNekdXU3V0aS9Yc0tYR0U0MThVSVdwTW0rbXdrUWxleEh2CmVrVEF6K21mTUhXOFExQjJZeXo2RFYvRWcvdVhqb3IzK0J5Q0lDNWpSYWJEUFhHSHVrWUYyOFJTWEhXc2drTkoKeWFjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.0.8:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:node1
  name: system:node:node1@kubernetes
current-context: system:node:node1@kubernetes
kind: Config
preferences: {}
users:
- name: system:node:node1
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJRENDQWdpZ0F3SUJBZ0lJYWVtUU1BbnVCSDh3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpFeE1qZ3hNak14TVRkYUZ3MHlOREV4TWpjeE1qTXhNakZhTURNeApGVEFUQmdOVkJBb1RESE41YzNSbGJUcHViMlJsY3pFYU1CZ0dBMVVFQXhNUmMzbHpkR1Z0T201dlpHVTZibTlrClpURXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFES1g4bHcrQU5xZ0tHb24rS24KandrNE5RMjJmeGNuM0V2cDBIT1VIQ2JCU2Y5RmdwQWZwbDZNUGR5VWN2S2h0MGxCOVFZWkVRbjdqQTUxNEd4RQplYWRPdlFTTzI1TVZKNUVhREJhMzFWdjl6K2EzNVNKUG9oaTkxVVdIWVVCcDZhRUJKZGtUMzNtaUVNU2JYRXhLCjdVNlpVbEFMRENuNXdmdmVzbkpKclN6bUgrc2Z4VDFSaVE3bTdQRThUYVN2eDhMek40bloraEI1QUpRdWMzZjYKVjZ3UnlaVmlxclY4b0UwcmJnSThFS2NoeUlXZ3FiRElkUUFuWmFWZHQxRFgyK2NscDlNVElUSFhZclZad1A3UQo2WitJaWhIOTR6bXBDaXFad2hDVXhHeGlUNGNMazBuK0tkOVRtZHpZMnBIczZ5d1JlYWdPMVcxWGdlaWNrbVAwCnY3dC9BZ01CQUFHalZqQlVNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0QKQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjhHQTFVZEl3UVlNQmFBRkk4aGx3d2dlVGphajYxQXpJK2l1bStkL1BNcgpNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUJSdFNYY2I3RUs5ekZaVERQemhzREt3b3FDZU5hTlp3YjBxVWlCCk94YW5xalFISkk3OUZGSG1IS0ZiZnI4cmZYNDRLUUQ0OUhuZTh1NVMydUU3L0R2ZlZjbHVRckJnNTRzbllsNU8KT1F1RmkxZ2ROZWorMjhhL1dEMkNaR1o4TXR2NmdUL3RWamtIdUxQbVRJR2Z3M1JST1dwMVprZzI2Uzg5WWdtdwozUXF1Y1Rhd0Y5SkNKa1NFelVoZ01RS1BaanFTR1VFRGplK0RGb3dBRTFURzJkakFJSmFjU2pWTkRwMHRYeFQ3CnFnZ3hjVnFobVFjTDJncy8wL1Q1cm5WQnE1ZHFSZ1JOZnpwT3dwWllhTDdEK0FYVzJ6WmZzMzYwQjRUWkhKaVAKL0w1dEJ3VjN2SDY1VXJNODdIQ2MyVU5SZTdtdUdqcVY3ZlgyenlwK0VLN2dzbC9qCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
[node1 kubernetes]$ cat $HOME/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1URXlPREV5TXpFeE4xb1hEVE16TVRFeU5URXlNekV4TjFvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTWdnCm1wVElSNy9JODFyVXl5dmVXaG51WElDUGN6bHM5amJNS3M1ZXcrazhqRy9ZcHdsNERjQzliM1U3bERXd1JtZFcKTWp1eFplNSsxMFpvSGYvb2hjc0V2MGtlUjU3ZTNTeXFubjVYa0gwVEdzZlZ0ZnZQdkh5SGQ2eE44T1N4RFRMMApKYWhpaWc1VzlYQ0VNdkxmK2I5cWRid2pqT0hvNUhQVXRuOEQ2cFZPT1E1aGFTN2lFdzE0dEpGRWNmZ2h3cGY4CkhrN2hYU1N3akNHSEYxUmdSanFpODcvUFpodG8zcDlFc0ZXTUV2M1VpNzNLejB3a2xtRWJpblVSeW92MS9xMUwKeFkxMUpRSUdwd2p4TE81UHExRTRJdkF4c0Z4bGVxS1Zpc2UzWUhmaTZmaXBLaktlOVRjd2MvVnRMRzJrdUpPKwpSNFp2UzJpODZzMXA2YlNVSUhVQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZJOGhsd3dnZVRqYWo2MUF6SStpdW0rZC9QTXJNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSTc5S2ZzWXlEemVLcWhkRGpQawpQaHp1RjYyek9kOHJ0bFB5REcrSEcvUFNkOU52Y0d1Z0R1cmRWUVZJa0R4UERMaGpFenF5NkF4RmRDMVh6ckM5CkNNSkhXN3NVYSt0Vm9tM0pncWVsOEZGQUFvWEpnNmp5cndMbFJ3emJmcXlLNVhGWEZpWWR0amN2RHJVSTdaSUwKWm41QTBKQW8xYjNodkpmZVBqY2cyaW1kQytrRjJ3YmVHcUpvOVpoaDArZ3VQMTEzOXZKNVFJdmI2aGQxUVVpNwo1TlhqMFNNMjVERHZJRmJSMElWZFpFNEViYVY4bjhNekdXU3V0aS9Yc0tYR0U0MThVSVdwTW0rbXdrUWxleEh2CmVrVEF6K21mTUhXOFExQjJZeXo2RFYvRWcvdVhqb3IzK0J5Q0lDNWpSYWJEUFhHSHVrWUYyOFJTWEhXc2drTkoKeWFjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.0.8:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJZUp1eE9zcHlmbmd3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpFeE1qZ3hNak14TVRkYUZ3MHlOREV4TWpjeE1qTXhNakJhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTBjVElmakRSMFg4bEhTZFMKeWllaFlSTTN0NXNEK1dvTkNmZ2ZQK0pEbGZDWjkvMHBCWkwyS1dyUFpjU0xxNzQyVm5TWklRMjNvY3gwS25IOQp1QUVNUUhQdEpCSUJYWWV5UHhBRjBrTTdGQlNaNDJmS1RJRndHNngxQmpxcTN0RnZMYy8vYXdkN1JMMGVTNW44CjZtR3JVTVppK3Bia0ROSmczcG8xOUFwUXJmQ28yTXBiNUxUL1NOSlVwZjlRU3c2QnJVRWxLSERxbE0rZ0tZcjAKckR5YWtMYkpNRlcxampod3pUakJPM3N0bTQ2R1JCYVBLdDhSeXVnZ290YW5UTTBJSXgyeko4eUtVcXlQdXRkZwpNM0kxMW42MkNDRklKWjVRRGwwYXVXVGZ2ZW5wbW1OamwyMW4wZzl0d0REMXFnVXVIbGdpYVpIRk9jQk9IRSs5CkorUnQxd0lEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTUElaY01JSGs0Mm8rdFFNeVBvcnB2bmZ6egpLekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBTVlQbVRDeVF5OG1JZkh5dG96d0w5Tlp6anVVUmZjdy9xdFdRClRsa3NYN0JDYkV6bnQ0aWlicE5Nc3FVUlluVENYbUR2VWx1My91VG9tS1FNU3QzR3lXNGF1VFpNRzFiek5MNlAKeGFaRDZ0ZFFrMWxGNlgwTHR0T05OVzE4NGhsdDNqYUExYmk1QWdSaER2bjZOdWVzQWtZeXZGVXFqR0V3VHh0bApPdHJVazZGem0wdS9RTlVLOSttai9IU3FTNlp5OXh6bEh0VkdZc0VWWnhwdTVVYVdqdWFWeEFvTGZqK0d5QXRHClB0bzJFWUowN2h1T3oyempldk5HSDFjK0FQMDd5bGdRVTc5ZG56TkFqYjd2UTltRDZMVXBFQmdncHlhOVk3TVEKM0laNEpZWDEvVGNPRmxIZWQyWkxMVTJrdDVkUGdBRmlWZGhrUExDaGFqZjl2Z3g4bHc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBMGNUSWZqRFIwWDhsSFNkU3lpZWhZUk0zdDVzRCtXb05DZmdmUCtKRGxmQ1o5LzBwCkJaTDJLV3JQWmNTTHE3NDJWblNaSVEyM29jeDBLbkg5dUFFTVFIUHRKQklCWFlleVB4QUYwa003RkJTWjQyZksKVElGd0c2eDFCanFxM3RGdkxjLy9hd2Q3UkwwZVM1bjg2bUdyVU1aaStwYmtETkpnM3BvMTlBcFFyZkNvMk1wYgo1TFQvU05KVXBmOVFTdzZCclVFbEtIRHFsTStnS1lyMHJEeWFrTGJKTUZXMWpqaHd6VGpCTzNzdG00NkdSQmFQCkt0OFJ5dWdnb3RhblRNMElJeDJ6Sjh5S1VxeVB1dGRnTTNJMTFuNjJDQ0ZJSlo1UURsMGF1V1RmdmVucG1tTmoKbDIxbjBnOXR3REQxcWdVdUhsZ2lhWkhGT2NCT0hFKzlKK1J0MXdJREFRQUJBb0lCQUVjUVBVUFMwaUlBeXRmMwpqR3FEeG40V2ZwZXE0a3ZLaTZSRzlIODE3ZU9MNFAxTVBHdHhBQWZEOFlMdWRNcTM0N3lpUDRZZEdZcXNzdVgyClBCZnI4dlNhMFVtWTkvRnhtUWNZTElDdzJnRFJnZGl5UHF4d1pVUmF4a0pYK2E4R2tIMmFxMUxjOC91emZlSHEKeHUyNExNdHVpZldPeks4elRqSGJXRS9XZHNLVjJYcDU0cjdlQWRvVUNoNlZYQzMwcEdLTkdDamZJVVF3b0JEYwpWbWM2aGRuSDZGQWFTMjNkRzN1R3BJSUN3ZTNqRjlxQ0hYNHhwc3JQTzU5OVZKZFBjNmw4SVd2cWZ6RU5UWk91CnExTi9LNEsvdERmQm5tSmR0eHg3eWR6WTJEbThFTCtLQjVsa3dmVy8vb2R5dUZ3aHU4K3M3Mjd2RFNMMDJGRUsKeDVJc3R3RUNnWUVBK3Yzb09vUmQ5UGQycWxSYXpHWTl2eWFOME5hU3ZIMVZFY1VVNEpWVGg0SVJEYmR1QjF2bgp0eXpiZk52VmlCTWNJYWVSeFFQMnJDRDRQL0o1ZG5Qb1gxK2g4dkJUUkUvS29HeTM1SVkyT2JDMjFrUXRWSFFkCjhreC9La1dRNE9odXlpWTdONzZHM3RBVTRjcXB5UWNKOXUyNjRLc1kwM21CR1VoNnA4akVFa0VDZ1lFQTFmUk4KeWJ4a0NkVWhPZnRPN1ZZbWxzUUdra01TS0cweW1sMzBFYkkzSzJmM0l2SmlIZHNqZktzL1p4aU8rbldNVVFtSApRZFcrTlF2QzdVV2FqaDhkQ0Rtc1lFSm4wNXhrMGxvdVdJZWpvZWt2Zjc3bml5S1JlOGZJZ3ZtQkplUjloQUVTCjJZeWNWUmR5OGVVajdBNWFLSWVwTUlyRG5YQ2VGbTgwWXpFQVNoY0NnWUJzT01MRFNJUEN0bGtDUkhFS0pTT0cKb2pjUHp4VmlGS0J6a3dqWGRVdVk2Q28yUWRzOWJSRlNXQk1qbVI1UEYvNVBQSk9MNFpPZzBHNTZGRER2TkRjRAp1Slo1eThuN2NIUXNDL1NXb3BBSysvazZGaGQ5akx2RTE5c1UvUDNsSEJONU1sYmlTRktiREJtVTZlK2ZjRnVKCmp5Tys3OGM2bCs5a1dXNFpubVY4Z1FLQmdRQ0RoMkcrMERycnRJTnRvcW45eThXRFQ5VCtXNDh4SFJoTHY5Q0kKaEhmWURsem10ZjFOZjJkTWYvWkZTZHF6N0VtcmsrRDd2d3Zram10eDlDVTBrN29FVVhnLzRQTDFLeUxzRWhhRgp4RFQ3RjJCazl4WmUrWXhZZlJRdU9IYUZ0ZmJYak5WcXVqOXRMREkwdFBvbFp4bFQ2em8rcGovbGJOTXkxWUJ1CkxqU1FWd0tCZ1FDeHNydUZVU01SYm5jM3JtSmJiNjR0a1FiNHNEckxDdEQ3V3NreHBHd0NQelEvNUNHdDFpODcKL3R2OFVOSDZScURwTkVuWjFKc2ErWG5wSXNNVDdscU1NeUlRWTFhZ29RTFhENmFpcU54TGJzWTczbmJpVWg1cgpMcDdRa0w5aXhxUGtsdHpZQ1F4Ni9EdjRxT015OExBSEs3Zlk5ZmcrZHBwSXp4OXp1U1VpTWc9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=

```




port 8080 못쓴다 자꾸 port 사용할 수 있냐 뜬다.. 살려줘




container 끼리 통신할 수 있도록 해주는 CNI

각각의 기능을 하는 앱들끼리 kuberouter로 통신지원
파드끼리 CNI로 물리네트웍에 연결 후 물리네트웍을 받은 쪽에서는cNI를 통해 컨테이너 간의 통신을 할 수 있게한다.

```
#node2

[node2 ~]$ kubeadm join 192.168.0.8:6443 --token kchuga.853j4tdkp2qj7q3j \
>         --discovery-token-ca-cert-hash sha256:fd07df1b80ac153eea30ba356525857ce5bc557910957160669dafe787f9dd98
Initializing machine ID from random generator.
W1128 12:32:54.105935     579 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/docker/containerd/containerd.sock". Please update your configuration!
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.4.0-210-generic
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_BLKIO: enabled
        [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "", err: exit status 1
        [WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.





#node1

[node1 ~]$ kubectl get nodes
NAME    STATUS     ROLES           AGE   VERSION
node1   NotReady   control-plane   36s   v1.27.2
# 노드 연결 후


[node1 ~]$ kubectl get nodes
NAME    STATUS     ROLES           AGE   VERSION
node1   NotReady   control-plane   92s   v1.27.2
node2   NotReady   <none>          6s    v1.27.2

```





``` 

# 기본 파드

[node1 kubernetes]$ kubectl get pods --all-namespaces
NAMESPACE     NAME                            READY   STATUS    RESTARTS      AGE
kube-system   coredns-5d78c9869d-2hdtr        1/1     Running   0             35m
kube-system   coredns-5d78c9869d-dhsrx        1/1     Running   0             35m
kube-system   etcd-node1                      1/1     Running   3 (32m ago)   35m
kube-system   kube-apiserver-node1            1/1     Running   0             35m
kube-system   kube-controller-manager-node1   1/1     Running   2 (32m ago)   35m
kube-system   kube-proxy-npr6z                1/1     Running   0             34m
kube-system   kube-proxy-tzkvm                1/1     Running   0             35m
kube-system   kube-router-6x8rc               1/1     Running   0             35m
kube-system   kube-router-ws5p4               1/1     Running   0             34m
kube-system   kube-scheduler-node1            1/1     Running   2 (32m ago)   35m

```




## 직접 설치

- skip



